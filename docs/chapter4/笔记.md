# LLM

## 4.2.3 RLHF
**RLHF** - Reinforcement Learning from Human Feedback

RM，Reward Model，即奖励模型
PPO，Proximal Policy Optimization，近端策略优化算法，是一种经典的 RL 算法.
