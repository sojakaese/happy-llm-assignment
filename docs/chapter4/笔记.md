# LLM 大语言模型

## 4.1.1 大语言模型的定义
大语言模型包含数百亿（或更多）参数的语言模型，它们往往在数T token 语料上通过多卡分布式集群进行预训练，具备远超出传统预训练模型的文本理解与生成能力。

## 4.1.2 LLM 的能力
这些独特能力是 LLM 区别于传统 PLM 的重要优势。

**涌现能力**（Emergent Abilities)
模型性能随着规模增大而迅速提升，超过了随机水平，也就是我们常说的量变引起了质变.

**上下文学习**（In-context Learning）
上下文学习是指允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。上下文学习也被称为**few-shot**

**指令遵循**（Instruction Following）
也叫**指令微调**， 是通过使用自然语言描述的多任务数据对模型进行微调，经过指令微调的 LLM 能够理解并遵循未见过的指令，并根据任务指令执行任务，而无需事先见过具体示例，这展示了其强大的泛化能力。比如，通过给 ChatGPT 输入指令，其可以写作文、编程序、批改试卷、阅读报纸，Agent、WorkFlow等等。

**逐步推理**（Step by Step Reasoning）
LLM 通过采用思维链（Chain-of-Thought，CoT）推理策略，可以利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。据推测，这种能力可能是通过对代码的训练获得的。逐步推理能力意味着 LLM 可以处理复杂逻辑任务，也就是说可以解决日常生活中需要逻辑判断的绝大部分问题。

## 4.1.3 LLM 的特点

**多语言支持** - Multilingual Support

**长文本处理** - Long Text Processing
如要求 LLM 读完《红楼梦》并写一篇对应的高考作文

**拓展多模态** - Multimodal Extension

**幻觉** - hallucination 
幻觉，是指 LLM 根据 Prompt 杜撰生成虚假、错误信息的表现

# 4.2 训练 LLM

## 4.2.1 Pretrain

## 4.2.2 SFT

## 4.2.3 RLHF
**RLHF** - Reinforcement Learning from Human Feedback

RM，Reward Model，即奖励模型
PPO，Proximal Policy Optimization，近端策略优化算法，是一种经典的 RL 算法。
