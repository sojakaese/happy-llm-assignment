# 任务3 第二章的笔记

### 2.1.2中词向量点积公式的补充

#### 📐 点积公式
$$
v \cdot w = \sum_{i} v_i w_i
$$

**公式解析**：
- **符号说明**：
  - `v` 和 `w`：两个相同维度的向量（如词向量）
  - `i`：向量的维度索引（词向量通常有数百维）
  - `v_i` 和 `w_i`：向量在第 `i` 维的分量值
- **计算过程**：
  1. 将两个向量**对应维度的值相乘**
  2. 对所有维度的乘积结果**求和**
- **几何意义**：  
  点积结果反映向量的方向相似性：  
  ✅ 正值 → 方向相近（相似度高）  
  ❌ 负值 → 方向相反（相似度低）  
  ⚠️ 零值 → 正交（无相关性）

---

#### 🔍 计算示例（二维简化模型）
假设两个词向量：
- **"快乐"** 向量：`v = [2, 3]`
- **"喜悦"** 向量：`w = [1, 4]`

**点积计算步骤**：
| 维度 | v_i | w_i | v_i × w_i |
|------|-----|-----|-----------|
| 1    | 2   | 1   | 2×1 = **2** |
| 2    | 3   | 4   | 3×4 = **12** |
| **求和** | | | `2 + 12 = 14` |

**结果分析**：
- 点积值 **14** → 表示两词有较高相似性（都含积极语义）

---

#### ⚖️ 对比案例：反义词计算
假设 **"悲伤"** 向量：`u = [-1, -2]`
- **"快乐" vs "悲伤"** 点积：  
  `(2×-1) + (3×-2) = -2 + (-6) = **-8**`  
  → 负值表明语义相反

---

#### 💡 词向量应用要点
| 度量方式 | 公式 | 特点 | 推荐场景 |
|----------|------|------|----------|
| **原始点积** | `v·w` | 计算快，但受向量长度影响 | 快速初步筛选 |
| **余弦相似度** | `\frac{v·w}{\|v\| \|w\|}` | 归一化到 [-1,1]，消除长度影响 | 精准相似性判断 |

**实际应用**：
```python
# Python计算示例 (NumPy)
import numpy as np

v = np.array([2, 3])   # "快乐"向量
w = np.array([1, 4])   # "喜悦"向量
u = np.array([-1, -2]) # "悲伤"向量

dot_vw = np.dot(v, w)  # 输出: 14
dot_vu = np.dot(v, u)  # 输出: -8

# 余弦相似度计算
cos_sim = dot_vw / (np.linalg.norm(v) * np.linalg.norm(w))  # ≈ 0.94
```

> **关键结论**：点积是词向量相似性的基础度量，但实际应用中常结合归一化处理（如余弦相似度）以提升准确性。


---

### 词向量之间的相似度
- **`q`**：**查询向量（Query）**，表示当前关注的位置（例如序列中的某个词）。
- **`K`**：**键矩阵（Key）**，包含序列中所有位置的键向量（用于被查询）。
- **`x = qK^T`**：  
  计算 **`q`** 与 **`K`** 中所有键向量的点积（相似度得分）。结果 `x` 是一个向量，表示当前查询 `q` 与序列中每个位置的相似程度。


`x = qK^T` 的结果会进一步处理：
1. **缩放（Scale）**：`x / sqrt(d_k)`（`d_k` 是向量维度，避免点积过大导致梯度不稳定）。
2. **Softmax 归一化**：将 `x` 转换为概率分布（和为 1），得到注意力权重。
3. **加权求和**：用权重对 **值向量（Value）** 加权，生成最终注意力输出：  
   `Output = Softmax(x) · V`


**物理意义**
- **`qK^T`** 本质是计算 **序列中所有位置对当前查询的贡献权重**。
- 这是 Transformer 能捕捉长距离依赖的核心：无论词间距多远，直接计算相似度。


### 图示说明
```plaintext
         Key Matrix (K)
        +----+----+----+
        | k1 | k2 | ... | kn |   (每列是一个键向量)
        +----+----+----+
             |
             | 点积运算 (qK^T)
             ↓
Query (q) → [x1, x2, ..., xn]   (相似度得分向量)
```

### 为何重要？
- **并行计算**：所有位置的 `qK^T` 可同时计算（高效）。
- **动态权重**：根据输入动态分配注意力（优于 RNN/CNN 的固定模式）。
- **核心创新**：使 Transformer 在 NLP 任务中取得突破性性能。

如需深入，可参考文档中的 **Scaled Dot-Product Attention** 部分（通常含完整公式和代码实现）。


