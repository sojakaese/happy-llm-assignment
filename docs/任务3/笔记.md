# 任务3 第二章的笔记

### 2.1.2中点积公式的补充

#### 📐 点积公式
$$
v \cdot w = \sum_{i} v_i w_i
$$

**公式解析**：
- **符号说明**：
  - `v` 和 `w`：两个相同维度的向量（如词向量）
  - `i`：向量的维度索引（词向量通常有数百维）
  - `v_i` 和 `w_i`：向量在第 `i` 维的分量值
- **计算过程**：
  1. 将两个向量**对应维度的值相乘**
  2. 对所有维度的乘积结果**求和**
- **几何意义**：  
  点积结果反映向量的方向相似性：  
  ✅ 正值 → 方向相近（相似度高）  
  ❌ 负值 → 方向相反（相似度低）  
  ⚠️ 零值 → 正交（无相关性）

---

#### 🔍 计算示例（二维简化模型）
假设两个词向量：
- **"快乐"** 向量：`v = [2, 3]`
- **"喜悦"** 向量：`w = [1, 4]`

**点积计算步骤**：
| 维度 | v_i | w_i | v_i × w_i |
|------|-----|-----|-----------|
| 1    | 2   | 1   | 2×1 = **2** |
| 2    | 3   | 4   | 3×4 = **12** |
| **求和** | | | `2 + 12 = 14` |

**结果分析**：
- 点积值 **14** → 表示两词有较高相似性（都含积极语义）

---

#### ⚖️ 对比案例：反义词计算
假设 **"悲伤"** 向量：`u = [-1, -2]`
- **"快乐" vs "悲伤"** 点积：  
  `(2×-1) + (3×-2) = -2 + (-6) = **-8**`  
  → 负值表明语义相反

---

#### 💡 词向量应用要点
| 度量方式 | 公式 | 特点 | 推荐场景 |
|----------|------|------|----------|
| **原始点积** | `v·w` | 计算快，但受向量长度影响 | 快速初步筛选 |
| **余弦相似度** | `\frac{v·w}{\|v\| \|w\|}` | 归一化到 [-1,1]，消除长度影响 | 精准相似性判断 |

**实际应用**：
```python
# Python计算示例 (NumPy)
import numpy as np

v = np.array([2, 3])   # "快乐"向量
w = np.array([1, 4])   # "喜悦"向量
u = np.array([-1, -2]) # "悲伤"向量

dot_vw = np.dot(v, w)  # 输出: 14
dot_vu = np.dot(v, u)  # 输出: -8

# 余弦相似度计算
cos_sim = dot_vw / (np.linalg.norm(v) * np.linalg.norm(w))  # ≈ 0.94
```

> **关键结论**：点积是词向量相似性的基础度量，但实际应用中常结合归一化处理（如余弦相似度）以提升准确性。
