# 第三章 预训练语言模型
在上一章里，大部分重要的语言模型的组件和机制都得到了解释，特别是transformer结构。本章介绍了三种预训练语言模型，这三种影响影响深远的模型通过使用基本组件和机制，通过不同的设计思路，实现了不同的预训练任务，从而实现了更强大的语言模型。

# 3.1 Encoder-only PLM - Google Bert
BERT (Bidirectional Encoder Representations from Transformers) 由 Google 团队在 2018年发布。

**BERT 核心思想包括：**

- Transformer 架构。
- 预训练+微调范式。同样在 2018年，ELMo 的诞生标志着预训练+微调范式的诞生。ELMo 模型基于双向 LSTM 架构，在训练数据上基于语言模型进行预训练，再针对下游任务进行微调，表现出了更加优越的性能，将 NLP 领域导向预训练+微调的研究思路。而 BERT 也采用了该范式，并通过将模型架构调整为 Transformer，引入更适合文本理解、能捕捉深层双向语义关系的预训练任务 MLM，将预训练-微调范式推向了高潮。

**Encoder Only:**
- 架构采取了 Transformer 的 Encoder 部分堆叠
- 针对于 NLU （Natural Language Understanding）任务打造的预训练模型
  - 输入一般是文本序列，而输出一般是 Label。例如情感分类的积极、消极。
  - 模型整体既是由 Embedding、Encoder 加上 prediction_heads 组成
    - Encoder Layer 都是和 Transformer 结构类似的层
    - Intermediate 层是是一个线性层加上激活函数 GELU
    - BERT的注意力机制
      - 融合了**相对位置编码**，将其视为可训练的权重参数。
      - 先通过 Position Embedding 层来融入相对位置信息。
      - 通过可训练的参数来拟合相对位置，相对绝对位置编码 Sinusoidal 能够拟合更丰富的相对位置信息，但增加了模型参数，并缩短了上下文长度的处理能力。
        
**预训练任务 MLM 和 NSP**
- 掩码语言模型 Masked Language Model (MLM) 
  - 模拟的是“完形填空”。在一个文本序列中随机遮蔽部分 token，然后将所有未被遮蔽的 token 输入模型，要求模型根据输入预测被遮蔽的 token。
  - 模型利用被遮蔽的 token 的上文和下文一起理解语义来预测被遮蔽的 token，因此可以拟合双向语义，实现文本的理解
  - 预训练任务无需对文本进行任何人为的标注，只需要对文本进行随机遮蔽即可，因此也可以利用互联网所有文本语料实现预训练。  
- 下一句预测 Next Sentence Prediction (NSP) 
  - 模拟的是“下一句预测”。要求模型判断一个句对的两个句子是否是连续的上下文
  - 通过要求模型判断句对关系，迫使模型拟合句子之间的关系，来适配句级的 NLU 任务。
  - 由于 NSP 的正样本可以从无监督语料中随机抽取任意连续的句子，而负样本可以对句子打乱后随机抽取（只需要保证不要抽取到原本就连续的句子就行），因此也可以具有几乎无限量的训练数据。
  
**微调任务是下游任务**
- 下游任务，包括情感分析、文本分类、命名实体识别等
  - 微调的策略，和训练时更新模型参数一致，只不过在特定的任务、更少的训练数据、更小的 batch_size 上进行训练，更新参数的幅度更小。
  - 绝大部分下游任务都可以直接使用 BERT 的输出。例如，对于文本分类任务，可以直接修改模型结构中的 prediction_heads 最后的分类头即可。对于序列标注等任务，可以集成 BERT 多层的隐含层向量再输出最后的标注结果。对于文本生成任务，也同样可以取 Encoder 的输出直接解码得到最终生成结果。

# 3.2 Encoder-Decoder PLM
Encoder-Decoder模型，通过引入Decoder部分来解决Bert的问题，比如 MLM 任务和下游任务微调的不一致性，以及无法处理超过模型训练长度的输入等，同时也为 NLP 领域带来了新的思路和方法。

## 3.2.1 Google T5
T5（Text-To-Text Transfer Transformer）是 Google 预训练语言模型，通过将所有 NLP 任务统一表示为文本到文本的转换问题，简化了模型设计和任务处理。T5 基于 Transformer 架构，包含编码器和解码器两个部分，使用自注意力机制和多头注意力捕捉全局依赖关系，利用相对位置编码处理长序列中的位置信息，并在每层中包含前馈神经网络进一步处理特征。

**模型结构Encoder-Decoder**

**预训练任务**

**大一统思想**

# 3.3 Decoder-only PLM