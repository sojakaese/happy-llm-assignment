# 第三章 预训练语言模型
在上一章里，大部分重要的语言模型的组件和机制都得到了解释，特别是transformer结构。本章介绍了三种预训练语言模型，这三种影响影响深远的模型通过使用基本组件和机制，通过不同的设计思路，实现了不同的预训练任务，从而实现了更强大的语言模型。

# 3.1 Encoder-only PLM - Google Bert
BERT (Bidirectional Encoder Representations from Transformers) 由 Google 团队在 2018年发布。

**BERT 核心思想包括：**

- Transformer 架构。
- 预训练+微调范式。同样在 2018年，ELMo 的诞生标志着预训练+微调范式的诞生。ELMo 模型基于双向 LSTM 架构，在训练数据上基于语言模型进行预训练，再针对下游任务进行微调，表现出了更加优越的性能，将 NLP 领域导向预训练+微调的研究思路。而 BERT 也采用了该范式，并通过将模型架构调整为 Transformer，引入更适合文本理解、能捕捉深层双向语义关系的预训练任务 MLM，将预训练-微调范式推向了高潮。

**Encoder Only:**
- 架构采取了 Transformer 的 Encoder 部分堆叠
- 针对于 NLU （Natural Language Understanding）任务打造的预训练模型
  - 输入一般是文本序列，而输出一般是 Label。例如情感分类的积极、消极。
  - 模型整体既是由 Embedding、Encoder 加上 prediction_heads 组成
    - Encoder Layer 都是和 Transformer 结构类似的层
    - Intermediate 层是是一个线性层加上激活函数 GELU
    - BERT的注意力机制
      - 融合了**相对位置编码**，将其视为可训练的权重参数。
      - 先通过 Position Embedding 层来融入相对位置信息。
      - 通过可训练的参数来拟合相对位置，相对绝对位置编码 Sinusoidal 能够拟合更丰富的相对位置信息，但增加了模型参数，并缩短了上下文长度的处理能力。
        
**预训练任务 MLM 和 NSP**
- 掩码语言模型 Masked Language Model (MLM) 
  - 模拟的是“完形填空”。在一个文本序列中随机遮蔽部分 token，然后将所有未被遮蔽的 token 输入模型，要求模型根据输入预测被遮蔽的 token。
  - 模型利用被遮蔽的 token 的上文和下文一起理解语义来预测被遮蔽的 token，因此可以拟合双向语义，实现文本的理解
  - 预训练任务无需对文本进行任何人为的标注，只需要对文本进行随机遮蔽即可，因此也可以利用互联网所有文本语料实现预训练。  
- 下一句预测 Next Sentence Prediction (NSP) 
  - 模拟的是“下一句预测”。要求模型判断一个句对的两个句子是否是连续的上下文
  - 通过要求模型判断句对关系，迫使模型拟合句子之间的关系，来适配句级的 NLU 任务。
  - 由于 NSP 的正样本可以从无监督语料中随机抽取任意连续的句子，而负样本可以对句子打乱后随机抽取（只需要保证不要抽取到原本就连续的句子就行），因此也可以具有几乎无限量的训练数据。
  
**微调任务是下游任务**
- 下游任务，包括情感分析、文本分类、命名实体识别等
  - 微调的策略，和训练时更新模型参数一致，只不过在特定的任务、更少的训练数据、更小的 batch_size 上进行训练，更新参数的幅度更小。
  - 绝大部分下游任务都可以直接使用 BERT 的输出。例如，对于文本分类任务，可以直接修改模型结构中的 prediction_heads 最后的分类头即可。对于序列标注等任务，可以集成 BERT 多层的隐含层向量再输出最后的标注结果。对于文本生成任务，也同样可以取 Encoder 的输出直接解码得到最终生成结果。

# 3.2 Encoder-Decoder PLM - Google T5
Encoder-Decoder模型，通过引入Decoder部分来解决Bert的问题，比如 MLM 任务和下游任务微调的不一致性，以及无法处理超过模型训练长度的输入等，同时也为 NLP 领域带来了新的思路和方法。

T5（Text-To-Text Transfer Transformer）是 Google 预训练语言模型，通过将所有 NLP 任务统一表示为文本到文本的转换问题，简化了模型设计和任务处理。T5 基于 Transformer 架构，包含编码器和解码器两个部分，使用自注意力机制和多头注意力捕捉全局依赖关系，利用相对位置编码处理长序列中的位置信息，并在每层中包含前馈神经网络进一步处理特征。

**模型结构Encoder-Decoder**
T5 则采用了 Encoder-Decoder 结构，其中编码器和解码器都是基于 Transformer 架构设计。LayerNorm 采用了 RMSNorm，通过计算每个神经元的均方根（Root Mean Square）来归一化每个隐藏层的激活值。

**预训练任务**
T5模型的预训练任务是 MLM，也称为BERT-style目标。具体来说，就是在输入文本中随机遮蔽15%的token，然后让模型预测这些被遮蔽的token。这个过程不需要标签，可以在大量未标注的文本上进行。

**大一统思想**
所有的 NLP 任务都可以统一为文本到文本的任务，这一思想在自然语言处理领域具有深远的影响。其设计理念是将所有不同类型的NLP任务（如文本分类、翻译、文本生成、问答等）转换为一个统一的格式：输入和输出都是纯文本。

例如：
```
对于文本分类任务，
输入可以是“classify: 这是一个很好的产品”，
输出是“正面”；其中任务描述前缀，明确指定当前任务的类型

对于翻译任务，
输入可以是“translate English to French: How are you?”, 
输出是“Comment ça va?”。
```

# 3.3 Decoder-only PLM
Decoder-Only 是 LLM 的基础架构。GPT（Generative Pre-Training Language Model）是由 OpenAI 团队于 2018年发布的预训练语言模型。GPT 提出了通用预训练的概念，也就是在海量无监督语料上预训练，进而在每个特定任务上进行微调。

## 3.3.1 OpenAI GPT

**GPT 选择使用了 Decoder 来进行模型结构的堆叠**

**预训练任务 CLM** 
Decoder-Only 模型选择了—因果语言模型，Casual Language Model
```
  对于一个输入目标序列长度为 256，期待输出序列长度为 256 的任务，模型会不断根据前 256 个 token、257个 token（输入+预测出来的第一个 token）...... 进行 256 次计算，最后生成一个序列长度为 512 的输出文本，这个输出文本前 256 个 token 为输入，后 256 个 token 就是我们期待的模型输出。
```

**GPT的发展**
- GPT-1 - Decoder-Only
- GPT-2 - 扩大了模型参数规模、将 Post-Norm 改为了 Pre-Norm, 增加了预训练数据集和模型体量, 以 zero-shot（零样本学习）为主要目标，不对模型进行微调，直接要求模型解决任务
- GPT-3 - 体量带来了**涌现能力**， 提出了 **few-shot** 的重要思想, 
- ChatGPT - 引入预训练-指令微调-人类反馈强化学习的三阶段训练

## 3.3.2 Meta LLamMa
Meta于2023年2月发布了LLaMA-1.

## 3.3.3 智谱 GLM
GLM 系列模型是由智谱开发的主流中文 LLM 之一，包括 ChatGLM1、2、3及 GLM-4 系列模型，覆盖了指令理解、代码生成等多种应用场景, 核心思路是在传统 CLM 预训练任务基础上，加入 MLM 思想，从而构建一个在 NLG 和 NLU 任务上都具有良好表现的统一模型。