# 第一章 NLP 基础概念的总结

## NLP的定义和历史
- NLP是一种让计算机理解、解释和生成人类语言的技术，涉及了人类语言的理解、分析、生成和交互。
- NLP发展历程经过早期探索（Turing test, Noam Chomsky Grammar），到符号主义和统计方法，再到机器学习时代，目前进入了Transformer时代。
  
## NLP的主要任务
- NLP的核心任务
  - 核心任务：
    - 分词 word segmentation, 
    - 词切分subword segementation,
    - 词性标注 part-of-speech Tagging, 
    - 文本分类 Text Classification, 涉及到将给定的文本自动分配到一个或多个预定义的类别中，包括但不限于情感分析、垃圾邮件检测、新闻分类、主题识别等。
    - 命名实体识别 Named Entity Recognition, 旨在自动识别文本中具有特定意义的实体，并将它们分类为预定义的类别，如人名、地点、组织、日期、时间等。
    - 关系抽取 Relation Extraction, 旨在从文本中识别实体之间的语义关系，如实体之间的因果关系、拥有关系、亲属关系、地理位置关系、属性之间的关系等。
    - 文本摘要 Text Summarization, 目的是生成一段简洁准确的摘要，来概括原文的主要内容。两大类：
      - 抽取式摘要（Extractive Summarization） 摘要中的信息完全来自原文
      - 生成式摘要（Abstractive Summarization 生成式摘要不仅涉及选择文本片段，还需要对这些片段进行重新组织和改写，并生成新的内容
    - 机器翻译 Machine Translation, 使用计算机程序将一种自然语言（源语言）自动翻译成另一种自然语言（目标语言）的过程
  - 高级任务：
    - 自动问答 Automatic Question Answering, NLP 领域中的一个高级任务，旨在使计算机能够理解自然语言提出的问题，并根据给定的数据源自动提供准确的答案。
      - 检索式问答（Retrieval-based QA）通过搜索引擎等方式从大量文本中检索答案
      - 知识库问答（Knowledge-based QA）通过结构化的知识库来回答问题
      - 社区问答（Community-based QA）依赖于用户生成的问答数据

## 文本表示 
**定义：目的是将人类语言的自然形式转化为计算机可以处理的形式，也就是将文本数据数字化，以便计算机可以理解和处理。**
  - 向量空间模型（Vector Space Model, VSM）,  文本表示方法，通过将文本（包括单词、句子、段落或整个文档）转换为高维空间中的向量来实现文本的数学化表示。在这个模型中，每个维度代表一个特征项（例如，字、词、词组或短语），而向量中的每个元素值代表该特征项在文本中的权重，这种权重通过特定的计算公式（如词频TF、逆文档频率TF-IDF等）来确定，反映了特征项在文本中的重要程度。
  
  - N-gram模型，一种基于统计的语言模型，广泛应用于语音识别、手写识别、拼写纠错、机器翻译和搜索引擎等众多任务。N-gram模型的核心思想是基于马尔可夫假设，即一个词的出现概率仅依赖于它前面的N-1个词。
  
  - Word2Vec是一种流行的词嵌入（Word Embedding）技术，由Tomas Mikolov等人在2013年提出。它是一种基于神经网络NNLM的语言模型，旨在通过学习词与词之间的上下文关系来生成词的密集向量表示。Word2Vec的核心思想是利用词在文本中的上下文信息来捕捉词之间的语义关系，从而使得语义相似或相关的词在向量空间中距离较近。
    - 两种架构：CBOW（Continuous Bag of Words）和Skip-Gram
      - 连续词袋模型CBOW(Continuous Bag of Words)是根据目标词上下文中的词对应的词向量, 计算并输出目标词的向量表示；
      - Skip-Gram模型与CBOW模型相反, 是利用目标词的向量表示计算上下文中的词向量. 实践验证CBOW适用于小型数据集, 而Skip-Gram在大型语料中表现更好。
    - 优点：相比于传统的高维稀疏表示（如One-Hot编码），Word2Vec生成的是低维（通常几百维）的密集向量，有助于减少计算复杂度和存储需求。Word2Vec模型能够捕捉到上下文中词与词之间的语义关系，比如”国王“和“王后”在向量空间中的位置会比较接近。Word2Vec模型也可以很好的泛化到未见过的词，因为它是基于上下文信息学习的，而不是基于词典。
    - 缺点：由于CBOW/Skip-Gram模型是基于局部上下文的，无法捕捉到长距离的依赖关系，缺乏整体的词与词之间的关系，因此在一些复杂的语义任务上表现不佳。
  
  - ELMo（Embeddings from Language Models）实现了一词多义、静态词向量到动态词向量的跨越式转变。
    - 先在大型语料库上训练语言模型，得到词向量模型，然后在特定任务上对模型进行微调，得到更适合该任务的词向量，ELMo首次将预训练思想引入到词向量的生成中，使用双向LSTM结构，能够捕捉到词汇的上下文信息，生成更加丰富和准确的词向量表示。
    - ELMo采用典型的两阶段过程: 
      - 第1个阶段是利用语言模型进行预训练; 
      - 第2个阶段是在做特定任务时, 从预训练网络中提取对应单词的词向量作为新特征补充到下游任务中。
      - 基于RNN的LSTM模型训练时间长, 特征提取是ELMo模型优化和提升的关键。

# 支线任务
- 详细了解Word2Vec
- 详细了解ELMo