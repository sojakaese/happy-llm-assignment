高斯误差线性单元激活函数（Gaussian Error Linear Unit，简称 GELU）是一种近年来在深度学习领域广泛应用的激活函数，特别是在自然语言处理模型（如Transformer、BERT）中表现突出[1][3][7]。

## 数学定义

GELU 的标准数学表达式为：
$$
\text{GELU}(x) = x \cdot \Phi(x)
$$
其中 $$\Phi(x)$$ 是输入 $$x$$ 的标准高斯分布累积分布函数（CDF），具体为：
$$
\Phi(x) = 0.5 \left(1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right)
$$
这里的 $$\text{erf}$$ 是误差函数[1][5][8]。

在实际工程中，常用如下近似公式计算 GELU：
$$
\text{GELU}(x) \approx 0.5x \left[1 + \tanh\left(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right)\right]
$$
这个近似形式便于高效实现[3][6][8]。

## 原理与特点

- **概率解释**：GELU 可看作对输入 $$x$$ 以概率 $$\Phi(x)$$ 进行“门控”，即输入越大，被“保留”激活的概率越高，体现了一种输入相关的随机正则化思想[2][4][6]。
- **平滑性**：与 ReLU 等激活函数相比，GELU 的输出曲线更加平滑，有助于模型训练的稳定性[1][3]。
- **非线性与非单调性**：GELU 具备较强的非线性建模能力，能够更好地拟合复杂关系[3][7]。
- **缓解梯度消失**：GELU 具有非饱和特性，有利于缓解深层网络的梯度消失问题[1][7]。

## 应用场景

GELU 激活函数已在多种深度学习模型中得到应用，尤其是在自然语言处理和高维复杂数据建模任务中，通常能带来比 ReLU、ELU 更优的性能[1][3][7]。

## 总结

高斯误差线性单元激活函数（GELU）是一种结合了概率门控和平滑非线性的激活函数，能够提升深度神经网络的表达能力和训练效果，已成为现代深度学习模型中的主流选择之一.